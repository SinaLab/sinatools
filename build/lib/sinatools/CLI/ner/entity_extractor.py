"""
About:
------
The ArabiNER tool carries out Named Entity Recognition (NER) utilizing the ArabiNER utility from the SinaTools suite. It identifies the named entities and provides a comprehensive analysis in JSON format if the input consists of text, or in a CSV file if the input is a directory of files.

Usage:
------
Below is the usage information that can be generated by running arabi_ner --help.

.. code-block:: none

    arabi_ner --text=INPUT_TEXT
    arabi_ner --dir=INPUT_FILE --output_csv=OUTPUT_FILE_NAME

Options:
--------

.. code-block:: none

  --text INPUT_TEXT
        The text that needs to be analyzed for Named Entity Recognition.
  --file INPUT_FILE
        File containing the text to be analyzed for Named Entity Recognition.
  --output_csv OUTPUT_FILE_NAME
        A file containing the tokenized text and its Named Entity tags.
Examples:
---------

.. code-block:: none

    entity_extractor --text "Your text here"
    entity_extractor --dir "/path/to/your/directory" --output_csv "output.csv"

Note:
-----

.. code-block:: none

    - Ensure that the text input is appropriately encoded in UTF-8 or compatible formats.
    - The tool returns results in JSON format with proper indentation for better readability.
    - The quality and accuracy of the analysis depend on the underlying capabilities of the ArabiNER utility.

"""

import argparse
import json
import pandas as pd
from sinatools.ner.entity_extractor import extract
from sinatools.utils.tokenizer import corpus_tokenizer
from sinatools.utils.tokenizers_words import simple_word_tokenize


def combine_tags(sentence):
    output = extract(sentence)
    return [word[1] for word in output]


def main():
    parser = argparse.ArgumentParser(description='NER Analysis using ArabiNER')

    parser.add_argument('--text', type=str, help='Text to be analyzed for Named Entity Recognition')
    parser.add_argument('--dir', type=str, help='dir containing the text files to be analyzed for Named Entity Recognition')
    parser.add_argument('--output_csv', type=str, help='Output CSV file to write the results')

    args = parser.parse_args()

    if args.text is not None:
        results = extract(args.text)
        # Print the results in JSON format
        print(json.dumps(results, ensure_ascii=False, indent=4))
    elif args.dir is not None:
        corpus_tokenizer(args.dir, args.output_csv)
        df = pd.read_csv(args.output_csv)
        df['NER tags'] = None
        i = 0

        result = df.drop_duplicates(subset=['Global Sentence ID', 'Sentence'])
        unique_sentences = result['Sentence'].to_numpy()

        for sentence in unique_sentences: 
            ner_tags = combine_tags(sentence) 
            if len(simple_word_tokenize(sentence)) > 300:
                print(" Length of this sentence is more than 300 word:  ", sentence)
                return
            
            df.loc[i:i+len(ner_tags)-1, 'NER tags'] = ner_tags 
            i = i + len(ner_tags)
        
        df.to_csv(args.output_csv, index=False) 
    else:    
        print("Error: Either --text or --file argument must be provided.")
        return


if __name__ == '__main__':
    main()